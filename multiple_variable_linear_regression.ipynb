{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3538278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ff8fe",
   "metadata": {},
   "source": [
    "Multiple variable linear regression equation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "Cost function with multiple variables:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "Gradient descent with multiple variables:\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "* $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ and $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ are gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221e80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, w, b ):\n",
    "    \"\"\"\n",
    "    Calculates gradient for linear regression\n",
    "    Args:\n",
    "        X (ndarray, (m,n)): data, m examples with n features\n",
    "        y (ndarray, (m,)) : target values\n",
    "        w (ndarray, (n,)) : model parameters (weights)\n",
    "        b (scalar)        : model parameter (bias)\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw (ndarray, (n,)): gradient of cost wrt to parameters w.\n",
    "        dj_db (scalar)       : gradient of cost wrt to parameter b.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        err = (np.dot(w,X[i]) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i,j] \n",
    "        dj_db = dj_db + err\n",
    "    \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0259ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calculates cost for multiple variable linear regression\n",
    "    Args:\n",
    "        X (ndarray, (m,n)): data, m examples with n features\n",
    "        y (ndarray, (m,)) : target values\n",
    "        w (ndarray, (n,)) : model parameters (weights)\n",
    "        b (scalar)        : model parameters (bias)\n",
    "    \n",
    "    Returns:\n",
    "        cost (scalar)     : cost (error between predictions and target values)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        p = (np.dot(w, X[i]) + b) ## prediction\n",
    "        cost = cost + (p - y[i])**2\n",
    "    cost = cost / (2 * m)\n",
    "    return cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent by updating w, b using alpha.\n",
    "    Args:\n",
    "        X    (ndarray, (m,n)): data, m examples with n features\n",
    "        y    (ndarray, (m,)) : target values\n",
    "        w_in (ndarray (n,))  : initial values for model parameters (weights)\n",
    "        b_in (scalar)        : initial value for model parameter (bias)\n",
    "        cost_fuction         : function that calculates cost\n",
    "        gradient_function    : function that calculates gradients\n",
    "        alpha (float)        : learning rate\n",
    "        num_iters (int)      : number of iterations to run gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        w (ndarray, (m,n))   : updated values for model parameters (weights)\n",
    "        b (scalar)           : updates value for model parameter (bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
